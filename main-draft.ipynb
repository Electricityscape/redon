{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Building a virtual assistant (VA) for producing a supercapacitor \n",
    "\n",
    "The VA (hackaton version*) is supposed to solve the following **tasks**:\n",
    "\n",
    "1. intelligently process scientific documents\n",
    " * pick relevant data (text, images, tables, metadata) \n",
    " * filter low-quality information \n",
    "2. create a domain-specific knowledge graph \n",
    "3. provide useful answers to user's queries.\n",
    "\n",
    "*(later - add recommender system, which will make suggestions on material and process optimization)\n",
    "\n",
    "## Step-by-Step Process to Build the VA\n",
    "\n",
    "### 1. Data Collection \n",
    "\n",
    "Task: search through ArXiv documents using keywords \"supercapacitor\", for now should be able to work with PDFs (later: structured text - HTML, etc.)\n",
    "\n",
    "Tools: `BeautifulSoup`, `Selenium`, APIs like ArXiv (later: CrossRef)\n",
    "\n",
    "(later - expand the Database to include a large corpus of relevant scientific documents, research papers, patents, and technical manuals from IEEE, ScienceDirect, PubMed, Google Scholar, patents databases and tech blogs\n",
    "+ add multilanguage support??)\n",
    "\n",
    "\n",
    "### 2. Document Processing\n",
    "\n",
    "Task: extract structured data (text, tables, images, meta) from the documents\n",
    "\n",
    "Tools: `PyMuPDF` (text, diagrams, figures, metadata), `pdfplumber`(text), `PDFMiner` (text), `Camelot` (tables), `Tabula` (tables).\n",
    "Preferred method: `PDFMiner` combined with RPN (Region Proposal Network) - for detecting layout objects -  headers, sections, images, figures, tables, references.  \n",
    "Tutorial: https://medium.com/@baptisteloquette.entr/langchain-arxiv-tutor-data-loading-c62f55af492d\n",
    "\n",
    "\n",
    "### 3. Information Filtering\n",
    "\n",
    "Task: filter relevant vs. low-quality information (later: remove redundant/ duplicate information)\n",
    "\n",
    "Tools: pre-trained LLMs: `SciBERT`, `PubMedBERT` (both fine-tuned on scientific documents) process domain-specific text, \n",
    "`TextRank`, `TF-IDF`, `BERT embeddings` to identify the most relevant sections, keywords, or sentences based on the query, (later: deduplication algorithms to remove similar or redundant information)\n",
    "\n",
    "### 4. Knowledge Representation (Domain-Specific Ontology)\n",
    "\n",
    "Task: build an ontology/ knowledge graph specific to supercapacitor development to map key concepts (e.g. electrode materials, electrolyte properties, fabrication processes) to the extracted information\n",
    "\n",
    "Tools: `Neo4j` or `RDF` (Resource Description Framework) to structure the extracted information, \n",
    "Named Entity Recognition (NER) for concept linking (to extract key concepts like materials (graphene, carbon nanotubes), methods (electrochemical synthesis), and metrics (energy density, power density)\n",
    "\n",
    "### 5. Contextual Understanding and Query System\n",
    "\n",
    "Task: create a query-based system where the user can ask questions/ make queries, e.g. \"What are the best electrode materials for high energy density?\", or \"Summarize recent developments in electrolyte materials for supercapacitors.\"\n",
    "\n",
    "Tools: Use a retrieval-based system `Haystack`, `ElasticSearch` combined with BERT-based Question Answering (QA) models (requires Hugging Face's transformers with pipeline(\"question-answering\")), use semantic (contextual) search to understand the intent of the query and retrieve the relevant sections from the documents\n",
    "\n",
    "### 6. Low-Quality Information Filtering\n",
    "\n",
    "Task: assess and filter low-quality information based on \n",
    "* citation count, journal impact, author/ institution credibility\n",
    "* technical jargon matching apers or documents lacking domain-specific language might be of lower relevance (use domain-specific vocabularies for filtering)\n",
    "* publication age (exclude older documents that may contain outdated methods or materials)\n",
    "\n",
    "### 7. Machine Learning for Data Extraction and Summarization\n",
    "\n",
    "Tasks: fine-tune models to perform information extraction and summarization\n",
    "\n",
    "Tools:\n",
    "`BERT`, `T5`, `GPT` to extract specific information (material properties, performance metrics, and manufacturing techniques), `BART`, `T5`, `PEGASUS` for automatic text summarization of relevant sections\n",
    "\n",
    "### 8. Image Recognition and Analysis\n",
    "Use Computer Vision techniques to analyze images (like SEM, TEM, or XRD patterns) relevant to supercapacitors:\n",
    "\n",
    "Object Detection: Detect key components (e.g., anode, cathode, electrolyte) in schematics or diagrams.\n",
    "OCR (Optical Character Recognition): Use tools like Tesseract for reading labels, legends, and other annotations in the images.\n",
    "9. Human-in-the-Loop\n",
    "Since automating the entire process might still be difficult, human-in-the-loop methods could be used. You could present the user with a summary of the extracted information and ask for feedback or adjustments.\n",
    "10. Implementing the Virtual Assistant\n",
    "Interface: Build a simple UI, possibly a chatbot, that can interact with the user. Frameworks like Flask or FastAPI can be used to host the model and process the requests.\n",
    "Chatbot API: Use Dialogflow or Rasa to build conversational agents that guide the user through the development process.\n",
    "Backend: Use Python and integrate models and tools (like SciBERT, Neo4j, etc.) for smooth query answering and document analysis.\n",
    "Tools and Libraries\n",
    "Text Processing: transformers, Spacy, nltk, PyMuPDF\n",
    "Tables: Camelot, Tabula\n",
    "Semantic Search/QA: Haystack, ElasticSearch, sentence-transformers\n",
    "Image Processing: PIL, OpenCV, Tesseract\n",
    "Ontologies/Graphs: Neo4j, rdflib\n",
    "Summarization: BART, T5, PEGASUS\n",
    "Deployment: Flask, FastAPI, Rasa\n",
    "Example: Building a Summarization Pipeline for Supercapacitor Data\n",
    "python\n",
    "Copy code\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load summarization model\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Sample extracted text from research papers\n",
    "extracted_text = \"\"\"\n",
    "Supercapacitors have been the focus of intense research in recent years due to their high power density and long cycle life. One of the key challenges in supercapacitor development is enhancing the energy density while maintaining high power density. The use of novel electrode materials such as graphene, carbon nanotubes, and metal oxides has shown promising results in this regard. Electrolytes play a crucial role in the performance of supercapacitors, with ionic liquids and solid-state electrolytes being explored for high-temperature applications...\n",
    "\"\"\"\n",
    "\n",
    "# Summarize the extracted text\n",
    "summary = summarizer(extracted_text, max_length=100, min_length=30, do_sample=False)\n",
    "print(summary[0]['summary_text'])\n",
    "Final Thoughts:\n",
    "Your virtual assistant can be designed to intelligently process both structured and unstructured data from scientific literature and guide you through the supercapacitor development process. By combining NLP, machine learning, and domain-specific ontologies, your VA can give highly targeted insights into materials, manufacturing processes, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# dependencies:\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import layoutparser as lp\n",
    "# ArXiv loader\n",
    "%pip install -qU langchain-community arxiv pymupdf\n",
    "from langchain.document_loaders import ArxivLoader, PDFMinerLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pdf2image import convert_from_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download papers\n",
    "\n",
    "def load_pdf(url):\n",
    "    paper_number    =   os.path.basename(url).strip(\".pdf\")\n",
    "    res             =   requests.get(url)\n",
    "    pdf_path        =   f\"papers/{paper_number}.pdf\"\n",
    "    with open(pdf_path, 'wb') as f:\n",
    "        f.write(res.content)\n",
    "    return paper_number\n",
    "\n",
    "link            =  \"https://arxiv.org/pdf/2306.08302.pdf\"\n",
    "paper_number    =   load_pdf(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs    =   PDFMinerLoader(f\"papers/{paper_number}.pdf\").load()\n",
    "\n",
    "# divide into blocks:\n",
    "text_splitter   =   RecursiveCharacterTextSplitter(\n",
    "    chunk_size=700, # Specify the character chunk size\n",
    "    chunk_overlap=0, # \"Allowed\" Overlap across chunks\n",
    "    length_function=len # Function used to evaluate the chunk size (in terms of characters)\n",
    ")\n",
    "\n",
    "docs    =   text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layout Parsing with LayoutParser\n",
    "\n",
    "Includes 3 components:\n",
    "1. a CNN to extract feature maps. \n",
    "2. a RPN (Region Proposal Network) that makes use of the feature maps to propose and refine a certain number of regions of interests. \n",
    "3. component gathering the best propositions, then refine them further, to produce a segmentation mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layout Parsing with LayoutParser\n",
    "\n",
    "def pdf_to_img(pdf_pth):\n",
    "    img_pth    =   os.path.join(\"papers_imgs\", os.path.basename(pdf_pth).strip(\".pdf\") + \"_imgs\")\n",
    "    if not os.path.exists(img_pth):\n",
    "        os.makedirs(img_pth)\n",
    "    images      =   convert_from_path(pdf_path=pdf_pth)\n",
    "    for i in range(len(images)):\n",
    "        images[i].save(os.path.join(img_pth, \"page\" + str(i) + \".jpg\"), \"JPEG\")\n",
    "    print(\"Images Saved !\")\n",
    "    return img_pth\n",
    "\n",
    "imgs_pth   =   pdf_to_img(f\"papers/{paper_number}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_publay    =   lp.Detectron2LayoutModel('lp://PubLayNet/mask_rcnn_X_101_32x8d_FPN_3x/config',\n",
    "                    extra_config=[\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\", 0.6],\n",
    "                    label_map={0: \"Text\", 1: \"Title\", 2: \"List\", 3:\"Table\", 4:\"Figure\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get layout of a page\n",
    "page_idx    =   6\n",
    "img_pth    =   os.path.join(pdf_pth, f\"page{page_idx}.jpg\")\n",
    "img         =   cv2.imread(img_path)\n",
    "img         =   img[..., ::-1]\n",
    "\n",
    "layout  =   model_publay.detect(img)\n",
    "lp.draw_box(img, layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coordinate(data):\n",
    "\n",
    "  x1 = data.block.x_1\n",
    "  y1 = data.block.y_1\n",
    "  x2 = data.block.x_2\n",
    "  y2 = data.block.y_2\n",
    "\n",
    "  return torch.tensor([[x1, y1, x2, y2]], dtype=torch.float)\n",
    "\n",
    "def get_iou(box_1, box_2):\n",
    "\n",
    "  return bops.box_iou(box_1, box_2)\n",
    "\n",
    "def get_area(bbox):\n",
    "  w = bbox[0, 2] - bbox[0, 0] # Width\n",
    "  h = bbox[0, 3] - bbox[0, 1] # Height\n",
    "  area  = w * h\n",
    "\n",
    "  return area\n",
    "\n",
    "def refine_bboxes(block_1, block_2):\n",
    "\n",
    "  bb1 = set_coordinate(block_1)\n",
    "  bb2 = set_coordinate(block_2)\n",
    "\n",
    "  iou = get_iou(bb1, bb2)\n",
    "\n",
    "  if iou.tolist()[0][0] != 0.0:\n",
    "\n",
    "    a1 = get_area(bb1)\n",
    "    a2 = get_area(bb2)\n",
    "\n",
    "    block_2.set(type='None', inplace= True) if a1 > a2 else block_1.set(type='None', inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle bboxes overlaps by computing the IntersectionOverUnion of each detected bboxes. If IoU > 0 then we have an overlap, we thus compute the area of the 2 overlapping bboxes, and only keep the bbox with the greatest area, by setting it’s type to \"None\" .\n",
    "\n",
    "Initialize the ocr_agent that will extract the text from the detected boxes, using Tesseract. Note that you can specify multiple languages with the following format : languages=[\"eng\", \"fra\"].\n",
    "Pass the image through the model. Only keeping the bboxes detected with labels “Text”, “Title”, or “List”. Ultimately excluding Figures.\n",
    "Sort the boxes by their positions on the page. Note that paper’s pages can be in 2 columns, we thus sort from top to bottom, then left to right\n",
    "Apply the refine_blocks function to handle bboxe’s overlap\n",
    "Infer the text for each bboxes, here we pad each images by 5. Padding improve the ocr_agent‘s accuracy.\n",
    "Append to a list the texts and the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_agent                   =   lp.TesseractAgent(languages=\"eng\")\n",
    "\n",
    "def extract_text_pdf_image_PubLay_OCR(img_path):\n",
    "    texts       =   []\n",
    "    image       =   cv2.imread(img_path)\n",
    "    image       =   image[..., ::-1]\n",
    "    layout      =   model_publay.detect(image)\n",
    "    text_blocks =   lp.Layout([b for b in layout if b.type in ['Text', 'List', 'Title']])\n",
    "\n",
    "    # Organize text blocks based on their positions on the page\n",
    "    h, w            =   image.shape[:2]\n",
    "    left_interval   =   lp.Interval(0, w/2*1.05, axis='x').put_on_canvas(image)\n",
    "    left_blocks     =   text_blocks.filter_by(left_interval, center=True)\n",
    "    left_blocks.sort(key = lambda b:b.coordinates[1], inplace=True)\n",
    "\n",
    "    right_blocks            =   lp.Layout([b for b in text_blocks if b not in left_blocks])\n",
    "    right_blocks.sort(key   =   lambda b:b.coordinates[1], inplace=True)\n",
    "\n",
    "    text_blocks = lp.Layout([b.set(id = idx) for idx, b in enumerate(left_blocks + right_blocks)])\n",
    "\n",
    "    for layout_i in text_blocks:    # If some of the blocks overlap -> Take the one with the most associated area\n",
    "        for layout_j in text_blocks:\n",
    "            if layout_i != layout_j:\n",
    "                refine_blocks(layout_i, layout_j)\n",
    "\n",
    "    for block in text_blocks:\n",
    "        segment_image = (block\n",
    "                        .pad(left=5, right=5, top=5, bottom=5)\n",
    "                        .crop_image(image))\n",
    "            # add padding in each image segment can help\n",
    "            # improve robustness \n",
    "            \n",
    "        text = ocr_agent.detect(segment_image)\n",
    "        block.set(text=text, inplace=True)\n",
    "    for l in text_blocks:\n",
    "        texts.append([l.text, l.type])\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_2_OCR(imgs_paths):\n",
    "    docs        =   []\n",
    "    for img_path_idx in range(len(os.listdir(imgs_paths))):\n",
    "        img_path        =   os.path.join(imgs_paths, \"page{}.jpg\".format(img_path_idx))\n",
    "        page_content    =   extract_text_pdf_image_PubLay_OCR(img_path)\n",
    "        for content in page_content:\n",
    "            text    =   content[0]\n",
    "            cat     =   content[1]\n",
    "            if \"REFERENCES\" in text and cat == \"Title\":  # exclude references\n",
    "                return docs\n",
    "            metadata        =   {\"page_number\" : img_path_idx, \"category\" : cat, \"source\" : paper_number}\n",
    "            docs.append(Document(page_content=text, metadata=metadata))\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacents_papers_urls       =   []\n",
    "adjacents_papers_numbers    =   []\n",
    "for doc in docs:\n",
    "    adjacents_papers_urls.extend([re.sub(\"abs\", \"pdf\", url) + \".pdf\" for url in re.findall(r'(https?://arxiv.org/abs\\S+)', doc.page_content)])\n",
    "    adjacents_papers_numbers.extend([re.findall('\\d{4}\\.\\d{5}', url)[0] for url in re.findall(r'(https?://arxiv.org/abs\\S+)', doc.page_content)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the papers:\n",
    "for pdf_number in adjacents_papers_numbers:\n",
    "    adj_docs    =   ArxivLoader(query=pdf_number)\n",
    "    adj_docs    =   PDFMinerLoader(f\"papers/{pdf_number}.pdf\").load()\n",
    "    adj_docs    =   text_splitter.split_documents(docs)\n",
    "    vdb_chunks.add_documents(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
